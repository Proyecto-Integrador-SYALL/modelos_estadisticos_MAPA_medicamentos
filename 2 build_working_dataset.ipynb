{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052835,
     "end_time": "2020-03-08T15:34:33.764089",
     "exception": false,
     "start_time": "2020-03-08T15:34:33.711254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cargar Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set(style=\"darkgrid\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conseguir Dataframe de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"base_dataset.csv\", index_col=\"patient_id\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_cols = [\"birth_date\", \"start_date\", \"start_night\", \"end_night\", \"measure_date_time\"]\n",
    "for col in interest_cols:\n",
    "    dataset[col] = pd.to_datetime(dataset[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"age\"] = (dataset[\"start_date\"] - dataset[\"birth_date\"]).apply(lambda x: x.total_seconds() / (365.25 * 24 * 60 * 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cols = [\n",
    "    'treatment',\n",
    "    'birth_date',\n",
    "    'start_date',\n",
    "    'start_night',\n",
    "    'end_night',\n",
    "    'gender',\n",
    "    'height',\n",
    "    'weight',\n",
    "    'age',\n",
    "    'smoking',\n",
    "    'excercise_frecuency',\n",
    "    'beer',\n",
    "    'wine',\n",
    "    'alcohol',\n",
    "    'strong_licor',\n",
    "]\n",
    "mean_cols = [\n",
    "    'sistolic',\n",
    "    'diastolic',\n",
    "    'heart_rate',\n",
    "    'mean_arterial_pressure',\n",
    "    'pulse_pressure',\n",
    "    'stroke_volume',\n",
    "    'cardiac_output',\n",
    "    'cardiac_index',\n",
    "    'systemic_vascular_resistance',\n",
    "    'pulse_wave_velocity'        \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataset_1 = dataset[max_cols].groupby(\"patient_id\").max()\n",
    "grouped_dataset_2 = dataset[mean_cols].groupby(\"patient_id\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataset = pd.merge(\n",
    "    grouped_dataset_1, \n",
    "    grouped_dataset_2, \n",
    "    how=\"inner\", \n",
    "    left_index=True, \n",
    "    right_index=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataset.to_csv('working_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear random Dataframe para compartir\n",
    "Como estos datos tienen informacion de los pacientes no se pueden compartir libremente. Para desarrollar el proyecto integrador, se genero una base de datos con la misma matriz de covarianza de la original, pero con datos aleatorizados bajo una distribucion gaussiana. Este permite el desarrollo de codigos por parte de los integrantes del grupo que despues seran usados en la base de datos final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'treatment',\n",
    "    'gender',\n",
    "    'smoking',\n",
    "    'excercise_frecuency',\n",
    "    'beer',\n",
    "    'wine',\n",
    "    'alcohol',\n",
    "    'strong_licor'\n",
    "]\n",
    "\n",
    "numerical_columns = [\n",
    "    'height',\n",
    "    'weight',\n",
    "    'age',\n",
    "    'sistolic',\n",
    "    'diastolic',\n",
    "    'heart_rate',\n",
    "    'mean_arterial_pressure',\n",
    "    'pulse_pressure',\n",
    "    'stroke_volume',\n",
    "    'cardiac_output',\n",
    "    'cardiac_index',\n",
    "    'systemic_vascular_resistance',\n",
    "    'pulse_wave_velocity'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacion de grupos.\n",
    "Este codigo lo que hace es generar datasets aleatorios en los que:\n",
    "- 1. filtrar por todas las posibles combinaciones de las variables cualitativas\n",
    "- 2. estandarizar este subgrupo\n",
    "- 3. calcular la matriz de covarianza de este subgrupo\n",
    "- 4. generar un array aleatorio de la misma cantidad de datos que el subgrupo \n",
    "- 5. transformar inversamente la estandarizacion\n",
    "- 6. repetir los pasos 2 al 5 por cada subgrupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_datasets = []\n",
    "for a in grouped_dataset[categorical_columns[0]].unique():\n",
    "    for b in grouped_dataset[categorical_columns[1]].unique()[:-1]:\n",
    "        for c in grouped_dataset[categorical_columns[2]].unique():\n",
    "            for d in grouped_dataset[categorical_columns[3]].unique():\n",
    "                for e in grouped_dataset[categorical_columns[4]].unique():\n",
    "                    for f in grouped_dataset[categorical_columns[5]].unique():\n",
    "                        for g in grouped_dataset[categorical_columns[6]].unique():\n",
    "                            for h in grouped_dataset[categorical_columns[7]].unique():\n",
    "                                random_dataset = grouped_dataset[\n",
    "                                    (grouped_dataset[categorical_columns[0]] == a) &\n",
    "                                    (grouped_dataset[categorical_columns[1]] == b) &\n",
    "                                    (grouped_dataset[categorical_columns[2]] == c) &\n",
    "                                    (grouped_dataset[categorical_columns[3]] == d) &\n",
    "                                    (grouped_dataset[categorical_columns[4]] == e) &\n",
    "                                    (grouped_dataset[categorical_columns[5]] == f) &\n",
    "                                    (grouped_dataset[categorical_columns[6]] == g) &\n",
    "                                    (grouped_dataset[categorical_columns[7]] == h)\n",
    "                                ].copy()\n",
    "                                std_random = StandardScaler()\n",
    "                                try:\n",
    "                                    std_random.fit(random_dataset[numerical_columns])\n",
    "                                except ValueError:\n",
    "                                    continue\n",
    "                                random_ = std_random.transform(random_dataset[numerical_columns])\n",
    "                                try:\n",
    "                                    random_array = np.random.multivariate_normal(random_.mean(axis=0), np.cov(random_,rowvar=False), random_.shape[0])\n",
    "                                except ValueError:\n",
    "                                    continue\n",
    "                                random_data = pd.DataFrame(std_random.inverse_transform(random_array), columns=numerical_columns)\n",
    "                                for col, val in list(zip(categorical_columns, [a, b, c, d, e, f, g, h])):\n",
    "                                    random_data[col] = val\n",
    "                                random_datasets.append(random_data)\n",
    "                           \n",
    "                            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenar los datasets aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = pd.concat(\n",
    "    random_datasets, \n",
    "    ignore_index=True\n",
    ")\n",
    "print(random_df.shape)\n",
    "random_df[\"index\"] = np.random.choice(random_df.shape[0],random_df.shape[0], False)\n",
    "random_df.set_index(\"index\",inplace=True)\n",
    "random_df.sort_index(inplace=True)\n",
    "random_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df.to_csv(\"random_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
